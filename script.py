"""
MediaPipe + YOLOv11n-pose GPUåŠ é€Ÿåµæ¸¬å™¨
- æ‰‹éƒ¨21å€‹é—œéµé» (MediaPipe + GPU)
- è‡‰éƒ¨248å€‹é—œéµé» (MediaPipe + GPU)  
- äººé«”17å€‹å§¿æ…‹é—œéµé» (YOLOv11n-pose + GPU)
- æ”¯æ´NVIDIA GPUåŠ é€Ÿ
"""

import cv2
import mediapipe as mp
import numpy as np
import time
import os
from datetime import datetime
from ultralytics import YOLO
import threading
from queue import Queue
import torch

class IntegratedLandmarkDetector:
    def __init__(self):
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–GPUåŠ é€Ÿåµæ¸¬å™¨...")
        
        # æª¢æŸ¥GPUå¯ç”¨æ€§
        self.check_gpu_availability()
        
        # åˆå§‹åŒ–MediaPipeè§£æ±ºæ–¹æ¡ˆ
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        self.mp_hands = mp.solutions.hands
        self.mp_face_mesh = mp.solutions.face_mesh
        
        # åˆå§‹åŒ–MediaPipeåµæ¸¬å™¨ï¼ˆGPUåŠ é€Ÿï¼‰
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5,
            model_complexity=0  # ä½¿ç”¨è¼ƒå¿«çš„æ¨¡å‹
        )
        
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,  # æ¸›å°‘åˆ°1å€‹è‡‰ä»¥æé«˜æ€§èƒ½
            refine_landmarks=False,  # 248å€‹é»
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # åˆå§‹åŒ–YOLOv11n-poseï¼ˆGPUåŠ é€Ÿï¼‰
        self.yolo_model = None
        self.yolo_load_attempts = 0
        self.max_load_attempts = 3
        
        print("âš¡ æ­£åœ¨è¼‰å…¥YOLOv11n-poseæ¨¡å‹ï¼ˆGPUåŠ é€Ÿï¼‰...")
        self.load_yolo_model()
        
        # å¤šç·šç¨‹è™•ç†
        self.use_threading = True
        self.frame_queue = Queue(maxsize=3)
        self.result_queue = Queue(maxsize=3)
        self.processing = False
        
        # COCOäººé«”é—œéµé»åç¨± (17å€‹é»)
        self.pose_keypoints = [
            "nose", "left_eye", "right_eye", "left_ear", "right_ear",
            "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
            "left_wrist", "right_wrist", "left_hip", "right_hip",
            "left_knee", "right_knee", "left_ankle", "right_ankle"
        ]
        
        # äººé«”éª¨æ¶é€£æ¥
        self.pose_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # é ­éƒ¨
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # ä¸ŠåŠèº«
            (5, 11), (6, 12), (11, 12),  # è»€å¹¹
            (11, 13), (13, 15), (12, 14), (14, 16)  # ä¸‹åŠèº«
        ]
        
        # æ‰‹éƒ¨21å€‹é—œéµé»çš„åç¨±
        self.hand_landmarks_names = [
            "WRIST", "THUMB_CMC", "THUMB_MCP", "THUMB_IP", "THUMB_TIP",
            "INDEX_FINGER_MCP", "INDEX_FINGER_PIP", "INDEX_FINGER_DIP", "INDEX_FINGER_TIP",
            "MIDDLE_FINGER_MCP", "MIDDLE_FINGER_PIP", "MIDDLE_FINGER_DIP", "MIDDLE_FINGER_TIP",
            "RING_FINGER_MCP", "RING_FINGER_PIP", "RING_FINGER_DIP", "RING_FINGER_TIP",
            "PINKY_MCP", "PINKY_PIP", "PINKY_DIP", "PINKY_TIP"
        ]
        
        # è¨­å®š
        self.show_hands = True
        self.show_face = True
        self.show_pose = True
        self.show_hand_connections = True
        self.show_face_mesh = True
        self.show_pose_connections = True
        self.show_landmarks_info = False  # é è¨­é—œé–‰ä»¥ä¿æŒç•«é¢æ¸…æ½”
        self.mirror_mode = True  # é¡åƒæ¨¡å¼é–‹é—œ
        
        # çµ±è¨ˆè³‡è¨Š
        self.hand_landmarks_count = 0
        self.face_landmarks_count = 0
        self.pose_landmarks_count = 0
        
        # å„²å­˜ç›¸é—œè¨­å®š
        self.output_dir = "output_media"
        self.auto_save = False
        self.video_writer = None
        self.save_interval = 30  # æ¯30å¹€å„²å­˜ä¸€æ¬¡æˆªåœ–
        self.frame_count = 0
        
        # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
        self.create_output_directory()
        
        # æ€§èƒ½å„ªåŒ–è¨­å®š
        self.skip_frames = 1  # æ¯éš”å¹¾å¹€è™•ç†ä¸€æ¬¡ï¼ˆ1=æ¯å¹€éƒ½è™•ç†ï¼‰
        self.frame_counter = 0
        self.resize_factor = 1.0  # åœ–ç‰‡ç¸®æ”¾å› å­ï¼ˆ1.0=åŸå°ºå¯¸ï¼‰
        self.last_results = None  # å¿«å–ä¸Šä¸€å¹€çµæœ
        
        print("âœ… GPUåŠ é€Ÿåµæ¸¬å™¨åˆå§‹åŒ–å®Œæˆï¼")
    
    def check_gpu_availability(self):
        """æª¢æŸ¥GPUå¯ç”¨æ€§"""
        print("ğŸ” æª¢æŸ¥GPUåŠ é€Ÿæ”¯æ´...")
        
        # æª¢æŸ¥CUDA
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… CUDAå¯ç”¨ï¼åµæ¸¬åˆ° {gpu_count} å€‹GPU")
            print(f"ğŸ® ä¸»GPU: {gpu_name}")
            self.device = 'cuda'
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPU")
            self.device = 'cpu'
        
        # æª¢æŸ¥MediaPipe GPUæ”¯æ´
        try:
            # MediaPipeæœƒè‡ªå‹•ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            print("âœ… MediaPipe GPUæ”¯æ´å·²å•Ÿç”¨")
        except Exception as e:
            print(f"âš ï¸  MediaPipe GPUæ”¯æ´æª¢æŸ¥å¤±æ•—: {e}")
    
    def load_yolo_model(self):
        """è¼‰å…¥YOLOæ¨¡å‹çš„å°ˆç”¨æ–¹æ³•ï¼ˆGPUåŠ é€Ÿï¼‰"""
        model_names = [
            'yolo11n-pose.pt',
            'yolov8n-pose.pt'
        ]
        
        for model_name in model_names:
            try:
                print(f"ğŸ”„ å˜—è©¦è¼‰å…¥: {model_name}")
                self.yolo_model = YOLO(model_name)
                
                # è¨­å®šç‚ºGPUè£ç½®
                if self.device == 'cuda':
                    self.yolo_model.to('cuda')
                    print(f"ğŸ® {model_name} å·²è¼‰å…¥åˆ°GPU")
                else:
                    print(f"ğŸ’» {model_name} ä½¿ç”¨CPU")
                
                # æ¸¬è©¦æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
                dummy_img = np.zeros((640, 640, 3), dtype=np.uint8)
                test_results = self.yolo_model(dummy_img, verbose=False, device=self.device)
                
                print(f"âœ… æˆåŠŸè¼‰å…¥ä¸¦æ¸¬è©¦: {model_name}")
                return True
                
            except Exception as e:
                print(f"âŒ {model_name} è¼‰å…¥å¤±æ•—: {str(e)[:100]}...")
                self.yolo_model = None
                continue
        
        print("âš ï¸  æ‰€æœ‰YOLOæ¨¡å‹è¼‰å…¥å¤±æ•—")
        return False
    
    def optimize_frame(self, frame):
        """å„ªåŒ–å¹€è™•ç†"""
        # å¦‚æœè¨­å®šäº†ç¸®æ”¾å› å­ï¼Œç¸®å°åœ–ç‰‡ä»¥æé«˜è™•ç†é€Ÿåº¦
        if self.resize_factor < 1.0:
            h, w = frame.shape[:2]
            new_h, new_w = int(h * self.resize_factor), int(w * self.resize_factor)
            frame = cv2.resize(frame, (new_w, new_h))
        
        return frame
    
    def restore_coordinates(self, results, original_shape):
        """å°‡ç¸®æ”¾å¾Œçš„åº§æ¨™é‚„åŸåˆ°åŸå§‹å°ºå¯¸"""
        if self.resize_factor >= 1.0:
            return results
        
        # é€™è£¡å¯ä»¥åŠ å…¥åº§æ¨™é‚„åŸé‚è¼¯
        # æš«æ™‚ç°¡åŒ–è™•ç†
        return results
        
    def draw_hand_landmarks(self, frame, hand_landmarks, handedness):
        """ç¹ªè£½æ‰‹éƒ¨é—œéµé»"""
        h, w, _ = frame.shape
        
        # ç¹ªè£½æ‰‹éƒ¨é€£æ¥ç·š
        if self.show_hand_connections:
            self.mp_drawing.draw_landmarks(
                frame,
                hand_landmarks,
                self.mp_hands.HAND_CONNECTIONS,
                self.mp_drawing_styles.get_default_hand_landmarks_style(),
                self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        # ç¹ªè£½é—œéµé»ç·¨è™Ÿå’Œåº§æ¨™ï¼ˆåƒ…åœ¨é–‹å•Ÿè³‡è¨Šæ¨¡å¼æ™‚ï¼‰
        if self.show_landmarks_info:
            for idx, landmark in enumerate(hand_landmarks.landmark):
                x = int(landmark.x * w)
                y = int(landmark.y * h)
                
                # ç¹ªè£½é»
                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)
                cv2.putText(frame, str(idx), (x + 5, y - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 0), 1)
        
        # é¡¯ç¤ºæ‰‹éƒ¨é¡å‹
        wrist = hand_landmarks.landmark[0]
        wrist_x, wrist_y = int(wrist.x * w), int(wrist.y * h)
        hand_type = handedness.classification[0].label
        cv2.putText(frame, f"{hand_type}", (wrist_x - 30, wrist_y - 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)
    
    def draw_face_landmarks(self, frame, face_landmarks):
        """ç¹ªè£½è‡‰éƒ¨248å€‹é—œéµé»"""
        h, w, _ = frame.shape
        
        # ç¹ªè£½è‡‰éƒ¨ç¶²æ ¼
        if self.show_face_mesh:
            # ç¹ªè£½ä¸»è¦è¼ªå»“
            self.mp_drawing.draw_landmarks(
                frame,
                face_landmarks,
                self.mp_face_mesh.FACEMESH_CONTOURS,
                landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=(0, 255, 255), thickness=1, circle_radius=1),
                connection_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=(0, 128, 255), thickness=1)
            )
            
            # ç¹ªè£½çœ¼éƒ¨ç´°ç¯€
            self.mp_drawing.draw_landmarks(
                frame,
                face_landmarks,
                self.mp_face_mesh.FACEMESH_LEFT_EYE,
                landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=(255, 0, 0), thickness=1, circle_radius=1),
                connection_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=(255, 0, 0), thickness=1)
            )
            
            self.mp_drawing.draw_landmarks(
                frame,
                face_landmarks,
                self.mp_face_mesh.FACEMESH_RIGHT_EYE,
                landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=(255, 0, 0), thickness=1, circle_radius=1),
                connection_drawing_spec=self.mp_drawing.DrawingSpec(
                    color=(255, 0, 0), thickness=1)
            )
    
    def draw_pose_landmarks(self, frame, pose_results):
        """ç¹ªè£½YOLOv11n-poseäººé«”é—œéµé»"""
        if pose_results is None:
            return
            
        for result in pose_results:
            if hasattr(result, 'keypoints') and result.keypoints is not None:
                # è™•ç†ä¸åŒçš„keypointsæ ¼å¼
                if hasattr(result.keypoints, 'data'):
                    keypoints_data = result.keypoints.data
                elif hasattr(result.keypoints, 'xy'):
                    keypoints_data = result.keypoints.xy
                else:
                    keypoints_data = result.keypoints
                
                # è½‰æ›ç‚ºnumpy arrayä»¥ä¾¿è™•ç†
                keypoints_data = keypoints_data.cpu().numpy() if hasattr(keypoints_data, 'cpu') else keypoints_data
                
                # æª¢æŸ¥æ˜¯å¦æœ‰åµæ¸¬åˆ°äºº
                if len(keypoints_data) == 0:
                    self.pose_landmarks_count = 0
                    cv2.putText(frame, "Pose: No person detected", (10, 90), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 128, 0), 2)
                    return
                
                # è™•ç†æ¯å€‹åµæ¸¬åˆ°çš„äºº
                for person_idx in range(len(keypoints_data)):
                    keypoints = keypoints_data[person_idx]
                    
                    # åµæ¸¬ä¸¦é¡¯ç¤ºé—œéµé»
                    valid_points = []
                    for i, point in enumerate(keypoints):
                        if len(point) >= 3:  # x, y, confidence
                            x, y, conf = point[0], point[1], point[2]
                        elif len(point) == 2:  # åªæœ‰x, y
                            x, y, conf = point[0], point[1], 1.0
                        else:
                            continue
                        
                        if conf > 0.3:  # ä¿¡å¿ƒåº¦é–¾å€¼
                            x, y = int(x), int(y)
                            valid_points.append((i, x, y, conf))
                            
                            # ç¹ªè£½é—œéµé»
                            cv2.circle(frame, (x, y), 5, (255, 0, 255), -1)  # ç´«è‰²é»
                            cv2.circle(frame, (x, y), 7, (255, 255, 255), 2)  # ç™½è‰²é‚Šæ¡†
                            
                            # é¡¯ç¤ºé—œéµé»ç·¨è™Ÿï¼ˆåƒ…åœ¨è³‡è¨Šæ¨¡å¼æ™‚ï¼‰
                            if self.show_landmarks_info:
                                cv2.putText(frame, f"{i}", (x + 8, y - 8), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
                                cv2.putText(frame, f"{conf:.2f}", (x + 8, y + 15), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 0), 1)
                    
                    # ç¹ªè£½éª¨æ¶é€£æ¥
                    if self.show_pose_connections and len(valid_points) > 0:
                        # å»ºç«‹é—œéµé»å­—å…¸ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
                        point_dict = {idx: (x, y, conf) for idx, x, y, conf in valid_points}
                        
                        for connection in self.pose_connections:
                            start_idx, end_idx = connection
                            
                            if start_idx in point_dict and end_idx in point_dict:
                                start_x, start_y, start_conf = point_dict[start_idx]
                                end_x, end_y, end_conf = point_dict[end_idx]
                                
                                # æª¢æŸ¥å…©å€‹é»çš„ä¿¡å¿ƒåº¦
                                if start_conf > 0.3 and end_conf > 0.3:
                                    # æ ¹æ“šé€£æ¥éƒ¨ä½ä½¿ç”¨ä¸åŒé¡è‰²
                                    if start_idx <= 4 or end_idx <= 4:  # é ­éƒ¨
                                        color = (0, 255, 255)  # é»ƒè‰²
                                    elif start_idx >= 11 or end_idx >= 11:  # ä¸‹åŠèº«
                                        color = (255, 0, 0)    # è—è‰²
                                    else:  # ä¸ŠåŠèº«
                                        color = (0, 255, 0)    # ç¶ è‰²
                                    
                                    cv2.line(frame, (start_x, start_y), (end_x, end_y), color, 3)
                    
                    # æ›´æ–°å§¿æ…‹é—œéµé»è¨ˆæ•¸
                    self.pose_landmarks_count = len(valid_points)
                    
                    # é¡¯ç¤ºåµæ¸¬æˆåŠŸ
                    if len(valid_points) > 0:
                        cv2.putText(frame, f"Pose: Person {person_idx+1} ({len(valid_points)}/17)", 
                                   (10, 90 + person_idx * 25), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
    
    def load_yolo_model(self):
        """è¼‰å…¥YOLOæ¨¡å‹çš„å°ˆç”¨æ–¹æ³•"""
        model_names = [
            'yolo11n-pose.pt',
            'yolov8n-pose.pt', 
            'yolov8s-pose.pt',
            'yolo11s-pose.pt'
        ]
        
        for model_name in model_names:
            try:
                print(f"ğŸ”„ å˜—è©¦è¼‰å…¥: {model_name}")
                self.yolo_model = YOLO(model_name)
                
                # æ¸¬è©¦æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
                dummy_img = np.zeros((640, 640, 3), dtype=np.uint8)
                test_results = self.yolo_model(dummy_img, verbose=False)
                
                print(f"âœ… æˆåŠŸè¼‰å…¥ä¸¦æ¸¬è©¦: {model_name}")
                return True
                
            except Exception as e:
                print(f"âŒ {model_name} è¼‰å…¥å¤±æ•—: {str(e)[:100]}...")
                self.yolo_model = None
                continue
        
        print("âš ï¸  æ‰€æœ‰YOLOæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå°‡åªä½¿ç”¨MediaPipeåŠŸèƒ½")
        print("è«‹æª¢æŸ¥ï¼š")
        print("1. ç¶²è·¯é€£æ¥æ˜¯å¦æ­£å¸¸")
        print("2. pip install ultralytics")
        print("3. å˜—è©¦æ‰‹å‹•ä¸‹è¼‰: python -c \"from ultralytics import YOLO; YOLO('yolo11n-pose.pt')\"")
        return False
    
    def debug_yolo_detection(self, frame):
        """YOLOåµæ¸¬é™¤éŒ¯æ–¹æ³•"""
        if self.yolo_model is None:
            return False
        
        try:
            print(f"ğŸ” YOLOé™¤éŒ¯ - è¼¸å…¥å½±åƒå°ºå¯¸: {frame.shape}")
            
            # é€²è¡Œåµæ¸¬
            results = self.yolo_model(frame, verbose=False, conf=0.1)  # é™ä½ä¿¡å¿ƒåº¦é–¾å€¼
            
            print(f"ğŸ” YOLOçµæœæ•¸é‡: {len(results)}")
            
            for i, result in enumerate(results):
                print(f"ğŸ” çµæœ {i}:")
                print(f"   - boxes: {result.boxes is not None if hasattr(result, 'boxes') else 'No boxes attr'}")
                print(f"   - keypoints: {result.keypoints is not None if hasattr(result, 'keypoints') else 'No keypoints attr'}")
                
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    kpts = result.keypoints
                    print(f"   - keypoints shape: {kpts.data.shape if hasattr(kpts, 'data') else 'No data'}")
                    print(f"   - keypoints type: {type(kpts)}")
                    
                    if hasattr(kpts, 'data'):
                        data = kpts.data
                        if len(data) > 0:
                            print(f"   - first person keypoints shape: {data[0].shape}")
                            # æª¢æŸ¥æœ‰æ•ˆé—œéµé»æ•¸é‡
                            valid_count = 0
                            for point in data[0]:
                                if len(point) >= 3 and point[2] > 0.1:
                                    valid_count += 1
                            print(f"   - valid keypoints (conf>0.1): {valid_count}/17")
                        else:
                            print("   - no person detected")
            
            return True
            
        except Exception as e:
            print(f"âŒ YOLOé™¤éŒ¯å¤±æ•—: {e}")
            return False
    
    def create_output_directory(self):
        """å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            print(f"âœ… å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾: {self.output_dir}")
        
        # å»ºç«‹å­è³‡æ–™å¤¾
        subdirs = ['images', 'videos', 'data']
        for subdir in subdirs:
            path = os.path.join(self.output_dir, subdir)
            if not os.path.exists(path):
                os.makedirs(path)
    
    def save_frame(self, frame, frame_type="detection"):
        """å„²å­˜å–®ä¸€å¹€åˆ°åœ–ç‰‡"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        filename = f"{frame_type}_{timestamp}.jpg"
        filepath = os.path.join(self.output_dir, "images", filename)
        
        cv2.imwrite(filepath, frame)
        print(f"ğŸ“¸ å·²å„²å­˜æˆªåœ–: {filename}")
        return filepath
    
    def save_detection_result_image(self, original_frame):
        """å„²å­˜åµæ¸¬çµæœåœ–ç‰‡ - åŒ…å«æ‰€æœ‰é—œéµé»æ¨™è¨»"""
        # è¤‡è£½åŸå§‹å½±åƒé€²è¡Œè™•ç†
        result_frame = original_frame.copy()
        
        # é‡æ–°é€²è¡Œåµæ¸¬ä¸¦ç¹ªè£½æ‰€æœ‰é—œéµé»
        result_frame = self.process_frame_for_output(result_frame)
        
        # åŠ å…¥è©³ç´°è³‡è¨Šæ¨™è¨»
        self.add_detailed_annotations(result_frame)
        
        # å„²å­˜çµæœåœ–ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"detection_result_{timestamp}.jpg"
        filepath = os.path.join(self.output_dir, "images", filename)
        
        cv2.imwrite(filepath, result_frame)
        print(f"ğŸ¯ å·²å„²å­˜åµæ¸¬çµæœåœ–ç‰‡: {filename}")
        return filepath
    
    def process_frame_for_output(self, frame):
        """å°ˆé–€ç”¨æ–¼è¼¸å‡ºçš„å¹€è™•ç† - åŒ…å«æ‰€æœ‰è¦–è¦ºåŒ–æ•ˆæœ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # æ‰‹éƒ¨åµæ¸¬ä¸¦ç¹ªè£½
        if self.show_hands:
            hands_results = self.hands.process(rgb_frame)
            if hands_results.multi_hand_landmarks and hands_results.multi_handedness:
                for hand_landmarks, handedness in zip(hands_results.multi_hand_landmarks, 
                                                    hands_results.multi_handedness):
                    # ç¹ªè£½æ‰‹éƒ¨é—œéµé»å’Œé€£æ¥ç·š
                    self.mp_drawing.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS,
                        self.mp_drawing_styles.get_default_hand_landmarks_style(),
                        self.mp_drawing_styles.get_default_hand_connections_style()
                    )
                    
                    # æ¨™è¨»æ‰‹éƒ¨é¡å‹å’Œé—œéµé»ç·¨è™Ÿ
                    h, w, _ = frame.shape
                    hand_type = handedness.classification[0].label
                    wrist = hand_landmarks.landmark[0]
                    wrist_x, wrist_y = int(wrist.x * w), int(wrist.y * h)
                    
                    # æ‰‹éƒ¨æ¨™ç±¤
                    cv2.putText(frame, f"{hand_type} Hand (21 points)", 
                               (wrist_x - 50, wrist_y - 25), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
                    
                    # æ¨™è¨»é‡è¦é—œéµé»ç·¨è™Ÿ
                    important_hand_points = [0, 4, 8, 12, 16, 20]  # æ‰‹è…•å’ŒæŒ‡å°–
                    for idx in important_hand_points:
                        landmark = hand_landmarks.landmark[idx]
                        x, y = int(landmark.x * w), int(landmark.y * h)
                        cv2.putText(frame, str(idx), (x + 5, y - 5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        # è‡‰éƒ¨åµæ¸¬ä¸¦ç¹ªè£½
        if self.show_face:
            face_results = self.face_mesh.process(rgb_frame)
            if face_results.multi_face_landmarks:
                for face_landmarks in face_results.multi_face_landmarks:
                    # ç¹ªè£½è‡‰éƒ¨è¼ªå»“
                    self.mp_drawing.draw_landmarks(
                        frame, face_landmarks, self.mp_face_mesh.FACEMESH_CONTOURS,
                        landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                            color=(0, 255, 255), thickness=1, circle_radius=1),
                        connection_drawing_spec=self.mp_drawing.DrawingSpec(
                            color=(0, 128, 255), thickness=1)
                    )
                    
                    # ç¹ªè£½çœ¼éƒ¨å’Œå˜´éƒ¨ç´°ç¯€
                    self.mp_drawing.draw_landmarks(
                        frame, face_landmarks, self.mp_face_mesh.FACEMESH_LEFT_EYE,
                        landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                            color=(255, 0, 0), thickness=1, circle_radius=1)
                    )
                    self.mp_drawing.draw_landmarks(
                        frame, face_landmarks, self.mp_face_mesh.FACEMESH_RIGHT_EYE,
                        landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                            color=(255, 0, 0), thickness=1, circle_radius=1)
                    )
                    
                    # æ¨™è¨»è‡‰éƒ¨è³‡è¨Š
                    cv2.putText(frame, "Face Mesh (248 points)", (50, 50), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # äººé«”å§¿æ…‹åµæ¸¬ä¸¦ç¹ªè£½
        if self.show_pose and self.yolo_model is not None:
            try:
                pose_results = self.yolo_model(frame, verbose=False, conf=0.3)
                for result in pose_results:
                    if hasattr(result, 'keypoints') and result.keypoints is not None:
                        # è™•ç†keypointsæ ¼å¼
                        if hasattr(result.keypoints, 'data'):
                            keypoints_data = result.keypoints.data
                        else:
                            keypoints_data = result.keypoints
                        
                        if len(keypoints_data.shape) == 3:
                            keypoints = keypoints_data[0]
                        else:
                            keypoints = keypoints_data
                        
                        keypoints = keypoints.cpu().numpy() if hasattr(keypoints, 'cpu') else keypoints
                        
                        # ç¹ªè£½é—œéµé»å’Œéª¨æ¶
                        valid_points = []
                        for i, point in enumerate(keypoints):
                            if len(point) >= 3:
                                x, y, conf = point[0], point[1], point[2]
                            elif len(point) == 2:
                                x, y, conf = point[0], point[1], 1.0
                            else:
                                continue
                            
                            if conf > 0.3:
                                x, y = int(x), int(y)
                                valid_points.append((i, x, y, conf))
                                
                                # ç¹ªè£½é—œéµé»
                                cv2.circle(frame, (x, y), 6, (255, 0, 255), -1)
                                cv2.circle(frame, (x, y), 8, (255, 255, 255), 2)
                                
                                # æ¨™è¨»é—œéµé»åç¨±
                                cv2.putText(frame, f"{i}:{self.pose_keypoints[i][:4]}", 
                                           (x + 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 
                                           0.4, (255, 255, 0), 1)
                        
                        # ç¹ªè£½éª¨æ¶é€£æ¥
                        point_dict = {idx: (x, y, conf) for idx, x, y, conf in valid_points}
                        for connection in self.pose_connections:
                            start_idx, end_idx = connection
                            if start_idx in point_dict and end_idx in point_dict:
                                start_x, start_y, start_conf = point_dict[start_idx]
                                end_x, end_y, end_conf = point_dict[end_idx]
                                
                                if start_conf > 0.3 and end_conf > 0.3:
                                    # ä¸åŒéƒ¨ä½ä¸åŒé¡è‰²
                                    if start_idx <= 4 or end_idx <= 4:
                                        color = (0, 255, 255)  # é ­éƒ¨-é»ƒè‰²
                                    elif start_idx >= 11 or end_idx >= 11:
                                        color = (255, 0, 0)    # ä¸‹åŠèº«-è—è‰²
                                    else:
                                        color = (0, 255, 0)    # ä¸ŠåŠèº«-ç¶ è‰²
                                    
                                    cv2.line(frame, (start_x, start_y), (end_x, end_y), color, 3)
                        
                        # æ¨™è¨»å§¿æ…‹è³‡è¨Š
                        if len(valid_points) > 0:
                            cv2.putText(frame, f"Pose Skeleton (17 points)", 
                                       (50, frame.shape[0] - 50), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
            except Exception as e:
                print(f"å§¿æ…‹è™•ç†éŒ¯èª¤: {e}")
        
        return frame
    
    def add_detailed_annotations(self, frame):
        """åŠ å…¥è©³ç´°çš„è¨»é‡‹è³‡è¨Š"""
        h, w = frame.shape[:2]
        
        # å»ºç«‹åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (w-350, 10), (w-10, 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # æ¨™é¡Œ
        cv2.putText(frame, "Detection Summary", (w-340, 35), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # åµæ¸¬çµ±è¨ˆ
        info_lines = [
            f"Hands: {self.hand_landmarks_count} (max 42 points)",
            f"Face: {'âœ“' if self.face_landmarks_count > 0 else 'âœ—'} (248 points)",
            f"Pose: {'âœ“' if self.pose_landmarks_count > 0 else 'âœ—'} (17 points)",
            f"Total Points: {self.hand_landmarks_count * 21 + self.face_landmarks_count + self.pose_landmarks_count}",
            "",
            "Legend:",
            "ğŸŸ¡ Head connections",
            "ğŸŸ¢ Upper body",
            "ğŸ”µ Lower body",
            "ğŸŸ£ Pose keypoints"
        ]
        
        for i, line in enumerate(info_lines):
            if line:
                color = (255, 255, 255)
                if "âœ“" in line:
                    color = (0, 255, 0)
                elif "âœ—" in line:
                    color = (0, 0, 255)
                elif line.startswith("Legend"):
                    color = (0, 255, 255)
                
                cv2.putText(frame, line, (w-335, 60 + i * 15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        # æ™‚é–“æˆ³è¨˜
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(frame, timestamp, (10, h-20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    def start_video_recording(self, frame_shape):
        """é–‹å§‹éŒ„å½±"""
        if self.video_writer is not None:
            self.stop_video_recording()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"landmark_detection_{timestamp}.mp4"
        filepath = os.path.join(self.output_dir, "videos", filename)
        
        # è¨­å®šè¦–è¨Šç·¨ç¢¼å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        h, w = frame_shape[:2]
        self.video_writer = cv2.VideoWriter(filepath, fourcc, 30.0, (w, h))
        
        print(f"ğŸ¥ é–‹å§‹éŒ„å½±: {filename}")
        return filepath
    
    def create_detection_video(self, duration_seconds=10):
        """å»ºç«‹åµæ¸¬çµæœå½±ç‰‡ - éŒ„è£½æŒ‡å®šæ™‚é–“é•·åº¦"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"detection_analysis_{timestamp}.mp4"
        filepath = os.path.join(self.output_dir, "videos", filename)
        
        # é–‹å•Ÿæ”å½±æ©Ÿ
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        # å–å¾—å½±ç‰‡åƒæ•¸
        ret, frame = cap.read()
        if not ret:
            print("âŒ ç„¡æ³•è®€å–æ”å½±æ©Ÿ")
            cap.release()
            return None
        
        # æ ¹æ“šé¡åƒæ¨¡å¼æ±ºå®šæ˜¯å¦ç¿»è½‰
        if self.mirror_mode:
            frame = cv2.flip(frame, 1)
        
        h, w = frame.shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(filepath, fourcc, 30.0, (w, h))
        
        print(f"ğŸ¬ æ­£åœ¨éŒ„è£½åµæ¸¬åˆ†æå½±ç‰‡ ({duration_seconds}ç§’)...")
        print(f"ğŸ“¹ é¡åƒæ¨¡å¼: {'é–‹å•Ÿ' if self.mirror_mode else 'é—œé–‰'}")
        
        frame_count = 0
        target_frames = duration_seconds * 30  # 30 FPS
        
        while frame_count < target_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ ¹æ“šé¡åƒæ¨¡å¼æ±ºå®šæ˜¯å¦ç¿»è½‰
            if self.mirror_mode:
                frame = cv2.flip(frame, 1)
            
            # è™•ç†åµæ¸¬ä¸¦ç¹ªè£½æ‰€æœ‰é—œéµé»
            processed_frame = self.process_frame_for_output(frame.copy())
            
            # åŠ å…¥è©³ç´°è¨»é‡‹
            self.add_detailed_annotations(processed_frame)
            
            # åŠ å…¥é€²åº¦æ¢
            progress = frame_count / target_frames
            bar_width = w - 40
            bar_height = 20
            bar_x, bar_y = 20, h - 50
            
            # èƒŒæ™¯æ¢
            cv2.rectangle(processed_frame, (bar_x, bar_y), 
                         (bar_x + bar_width, bar_y + bar_height), (50, 50, 50), -1)
            # é€²åº¦æ¢
            cv2.rectangle(processed_frame, (bar_x, bar_y), 
                         (bar_x + int(bar_width * progress), bar_y + bar_height), 
                         (0, 255, 0), -1)
            # é€²åº¦æ–‡å­—
            cv2.putText(processed_frame, f"Recording: {frame_count}/{target_frames}", 
                       (bar_x, bar_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # å¯«å…¥å½±ç‰‡
            video_writer.write(processed_frame)
            frame_count += 1
            
            # é¡¯ç¤ºé è¦½
            cv2.imshow('Recording Detection Video', processed_frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        # æ¸…ç†
        video_writer.release()
        cap.release()
        cv2.destroyWindow('Recording Detection Video')
        
        print(f"âœ… åµæ¸¬åˆ†æå½±ç‰‡å·²å„²å­˜: {filename}")
        return filepath
    
    def stop_video_recording(self):
        """åœæ­¢éŒ„å½±"""
        if self.video_writer is not None:
            self.video_writer.release()
            self.video_writer = None
            print("â¹ï¸  éŒ„å½±å·²åœæ­¢")
    
    def save_video_frame(self, frame):
        """å„²å­˜è¦–è¨Šå¹€"""
        if self.video_writer is not None:
            self.video_writer.write(frame)
    
    def process_frame(self, frame):
        """è™•ç†å–®ä¸€å¹€ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        # å¹€è·³èºå„ªåŒ–
        self.frame_counter += 1
        if self.frame_counter % self.skip_frames != 0 and self.last_results is not None:
            # ä½¿ç”¨ä¸Šä¸€å¹€çš„çµæœ
            return self.apply_cached_results(frame, self.last_results)
        
        # å„ªåŒ–å¹€å°ºå¯¸
        optimized_frame = self.optimize_frame(frame)
        rgb_frame = cv2.cvtColor(optimized_frame, cv2.COLOR_BGR2RGB)
        
        # é‡ç½®è¨ˆæ•¸å™¨
        self.hand_landmarks_count = 0
        self.face_landmarks_count = 0
        self.pose_landmarks_count = 0
        
        results = {}
        
        # æ‰‹éƒ¨åµæ¸¬ (MediaPipe + GPU)
        if self.show_hands:
            hands_results = self.hands.process(rgb_frame)
            results['hands'] = hands_results
            if hands_results.multi_hand_landmarks and hands_results.multi_handedness:
                self.hand_landmarks_count = len(hands_results.multi_hand_landmarks)
                
                for hand_landmarks, handedness in zip(hands_results.multi_hand_landmarks, 
                                                    hands_results.multi_handedness):
                    self.draw_hand_landmarks(frame, hand_landmarks, handedness)
        
        # è‡‰éƒ¨åµæ¸¬ (MediaPipe + GPU)
        if self.show_face:
            face_results = self.face_mesh.process(rgb_frame)
            results['face'] = face_results
            if face_results.multi_face_landmarks:
                self.face_landmarks_count = len(face_results.multi_face_landmarks[0].landmark)
                
                for face_landmarks in face_results.multi_face_landmarks:
                    self.draw_face_landmarks(frame, face_landmarks)
                
                cv2.putText(frame, f"Face: {len(face_results.multi_face_landmarks)} detected", 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            else:
                cv2.putText(frame, "Face: Not detected", (10, 60), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                self.face_landmarks_count = 0
        
        # äººé«”å§¿æ…‹åµæ¸¬ (YOLOv11n-pose + GPU)
        if self.show_pose:
            if self.yolo_model is not None:
                try:
                    # GPUåŠ é€Ÿæ¨ç†
                    pose_results = self.yolo_model(
                        optimized_frame, 
                        verbose=False, 
                        conf=0.15,  # ç¨å¾®æé«˜ä¿¡å¿ƒåº¦ä»¥æ¸›å°‘èª¤æª¢
                        iou=0.5,
                        device=self.device,
                        half=self.device == 'cuda'  # ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿï¼ˆåƒ…GPUï¼‰
                    )
                    
                    results['pose'] = pose_results
                    
                    if pose_results and len(pose_results) > 0:
                        self.draw_pose_landmarks(frame, pose_results)
                    else:
                        self.pose_landmarks_count = 0
                        cv2.putText(frame, "Pose: No results", (10, 90), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                        
                except Exception as e:
                    self.pose_landmarks_count = 0
                    cv2.putText(frame, f"Pose: Error", (10, 90), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            else:
                self.pose_landmarks_count = 0
                cv2.putText(frame, "Pose: Model not loaded", (10, 90), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)
        
        # å¿«å–çµæœ
        self.last_results = results
        
        return frame
    
    def apply_cached_results(self, frame, cached_results):
        """æ‡‰ç”¨å¿«å–çš„åµæ¸¬çµæœ"""
        # æ‰‹éƒ¨çµæœ
        if 'hands' in cached_results and cached_results['hands'] and self.show_hands:
            hands_results = cached_results['hands']
            if hands_results.multi_hand_landmarks and hands_results.multi_handedness:
                for hand_landmarks, handedness in zip(hands_results.multi_hand_landmarks, 
                                                    hands_results.multi_handedness):
                    self.draw_hand_landmarks(frame, hand_landmarks, handedness)
        
        # è‡‰éƒ¨çµæœ
        if 'face' in cached_results and cached_results['face'] and self.show_face:
            face_results = cached_results['face']
            if face_results.multi_face_landmarks:
                for face_landmarks in face_results.multi_face_landmarks:
                    self.draw_face_landmarks(frame, face_landmarks)
        
        # å§¿æ…‹çµæœ
        if 'pose' in cached_results and cached_results['pose'] and self.show_pose:
            pose_results = cached_results['pose']
            if pose_results and len(pose_results) > 0:
                self.draw_pose_landmarks(frame, pose_results)
        
        return frame
    
    def add_info_panel(self, frame):
        """æ·»åŠ è³‡è¨Šé¢æ¿"""
        h, w = frame.shape[:2]
        
        # åªé¡¯ç¤ºFPSï¼ˆå³ä¸Šè§’ï¼‰
        if hasattr(self, 'fps'):
            cv2.putText(frame, f"FPS: {self.fps:.1f}", (w - 100, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # åœ¨å·¦ä¸Šè§’é¡¯ç¤ºåµæ¸¬ç‹€æ…‹å’Œæ€§èƒ½è³‡è¨Š
        status_text = []
        if self.show_hands:
            status_text.append(f"Hands: {self.hand_landmarks_count}")
        if self.show_face:
            status_text.append(f"Face: {'OK' if self.face_landmarks_count > 0 else 'NO'}")
        if self.show_pose:
            status_text.append(f"Pose: {'OK' if self.pose_landmarks_count > 0 else 'NO'}")
        
        # GPUç‹€æ…‹
        status_text.append(f"Device: {self.device.upper()}")
        
        # æ€§èƒ½è¨­å®š
        if self.skip_frames > 1:
            status_text.append(f"Skip: 1/{self.skip_frames}")
        if self.resize_factor < 1.0:
            status_text.append(f"Scale: {self.resize_factor:.1f}")
        
        # åŠ å…¥éŒ„å½±ç‹€æ…‹å’Œé¡åƒç‹€æ…‹
        if self.video_writer is not None:
            status_text.append("ğŸ”´ Recording")
        if not self.mirror_mode:
            status_text.append("ğŸ“¹ Normal View")
        
        for i, text in enumerate(status_text):
            color = (0, 0, 255) if "ğŸ”´" in text else (255, 255, 255)
            if "ğŸ“¹" in text:
                color = (0, 255, 255)
            elif "CUDA" in text:
                color = (0, 255, 0)
            elif "CPU" in text:
                color = (0, 165, 255)
            cv2.putText(frame, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
    
    def run_camera(self):
        """åŸ·è¡Œæ”å½±æ©Ÿåµæ¸¬"""
        cap = cv2.VideoCapture(0)
        
        # è¨­å®šæ”å½±æ©Ÿåƒæ•¸ï¼ˆå„ªåŒ–æ€§èƒ½ï¼‰
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 60)  # å˜—è©¦è¨­å®šæ›´é«˜çš„FPS
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # æ¸›å°‘ç·©è¡å€ä»¥é™ä½å»¶é²
        
        print("MediaPipe + YOLOv11n-pose GPUåŠ é€Ÿåµæ¸¬å™¨å·²å•Ÿå‹•ï¼")
        print("=" * 60)
        print("GPUåŠ é€ŸåŠŸèƒ½ï¼š")
        print(f"- é‹ç®—è£ç½®: {self.device.upper()}")
        print("- MediaPipe GPUåŠ é€Ÿ: è‡ªå‹•å•Ÿç”¨")
        print("- YOLOåŠç²¾åº¦æ¨ç†: " + ("å•Ÿç”¨" if self.device == 'cuda' else "ä¸æ”¯æ´"))
        print("- å¹€è·³èºå„ªåŒ–: " + ("å•Ÿç”¨" if self.skip_frames > 1 else "é—œé–‰"))
        print("")
        print("åµæ¸¬åŠŸèƒ½ï¼š")
        print("- æ‰‹éƒ¨åµæ¸¬ï¼š21å€‹é—œéµé» Ã— æœ€å¤š2éš»æ‰‹ (MediaPipe)")
        print("- è‡‰éƒ¨åµæ¸¬ï¼š248å€‹é—œéµé» (MediaPipe)")
        print("- äººé«”å§¿æ…‹ï¼š17å€‹é—œéµé» (YOLOv11n-pose)")
        print("")
        print("æ§åˆ¶éµï¼š")
        print("  H - åˆ‡æ›æ‰‹éƒ¨åµæ¸¬")
        print("  F - åˆ‡æ›è‡‰éƒ¨åµæ¸¬")
        print("  P - åˆ‡æ›äººé«”å§¿æ…‹åµæ¸¬")
        print("  C - åˆ‡æ›æ‰‹éƒ¨é€£æ¥ç·š")
        print("  M - åˆ‡æ›è‡‰éƒ¨ç¶²æ ¼")
        print("  S - åˆ‡æ›å§¿æ…‹éª¨æ¶")
        print("  I - åˆ‡æ›é—œéµé»è³‡è¨Š")
        print("  R - é–‹å§‹/åœæ­¢éŒ„å½±")
        print("  A - åˆ‡æ›è‡ªå‹•æˆªåœ–")
        print("  SPACE - æ‰‹å‹•æˆªåœ–")
        print("  D - å„²å­˜åµæ¸¬çµæœåœ–ç‰‡")
        print("  V - éŒ„è£½10ç§’åµæ¸¬åˆ†æå½±ç‰‡")
        print("  T - YOLOæ¨¡å‹é™¤éŒ¯æ¸¬è©¦")
        print("  X - åˆ‡æ›é¡åƒæ¨¡å¼")
        print("  1-5 - èª¿æ•´å¹€è·³èº (1=æœ€é«˜å“è³ª, 5=æœ€é«˜é€Ÿåº¦)")
        print("  Q - é€€å‡ºç¨‹å¼")
        print("-" * 60)
        
        prev_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("ç„¡æ³•è®€å–æ”å½±æ©Ÿç•«é¢")
                break
            
            # æ°´å¹³ç¿»è½‰ç•«é¢ï¼ˆå¯é¸çš„é¡åƒæ¨¡å¼ï¼‰
            if self.mirror_mode:
                frame = cv2.flip(frame, 1)
            
            # è™•ç†é—œéµé»åµæ¸¬
            frame = self.process_frame(frame)
            
            # è¨ˆç®—FPS
            current_time = time.time()
            self.fps = 1.0 / (current_time - prev_time)
            prev_time = current_time
            
            # æ·»åŠ è³‡è¨Šé¢æ¿
            self.add_info_panel(frame)
            
            # è‡ªå‹•å„²å­˜åŠŸèƒ½
            self.frame_count += 1
            
            # å„²å­˜è¦–è¨Šå¹€
            if self.video_writer is not None:
                self.save_video_frame(frame)
            
            # è‡ªå‹•æˆªåœ–ï¼ˆæ¯30å¹€ä¸€æ¬¡ï¼‰
            if self.auto_save and self.frame_count % self.save_interval == 0:
                if self.hand_landmarks_count > 0 or self.face_landmarks_count > 0 or self.pose_landmarks_count > 0:
                    self.save_frame(frame, "auto_detection")
            
            # é¡¯ç¤ºç•«é¢
            cv2.imshow('MediaPipe + YOLOv11n-pose Detector', frame)
            
            # æŒ‰éµæ§åˆ¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == ord('Q'):
                break
            elif key == ord('h') or key == ord('H'):
                self.show_hands = not self.show_hands
                print(f"æ‰‹éƒ¨åµæ¸¬: {'é–‹å•Ÿ' if self.show_hands else 'é—œé–‰'}")
            elif key == ord('f') or key == ord('F'):
                self.show_face = not self.show_face
                print(f"è‡‰éƒ¨åµæ¸¬: {'é–‹å•Ÿ' if self.show_face else 'é—œé–‰'}")
            elif key == ord('p') or key == ord('P'):
                self.show_pose = not self.show_pose
                print(f"äººé«”å§¿æ…‹åµæ¸¬: {'é–‹å•Ÿ' if self.show_pose else 'é—œé–‰'}")
            elif key == ord('c') or key == ord('C'):
                self.show_hand_connections = not self.show_hand_connections
                print(f"æ‰‹éƒ¨é€£æ¥ç·š: {'é–‹å•Ÿ' if self.show_hand_connections else 'é—œé–‰'}")
            elif key == ord('m') or key == ord('M'):
                self.show_face_mesh = not self.show_face_mesh
                print(f"è‡‰éƒ¨ç¶²æ ¼: {'é–‹å•Ÿ' if self.show_face_mesh else 'é—œé–‰'}")
            elif key == ord('s') or key == ord('S'):
                self.show_pose_connections = not self.show_pose_connections
                print(f"å§¿æ…‹éª¨æ¶: {'é–‹å•Ÿ' if self.show_pose_connections else 'é—œé–‰'}")
            elif key == ord('i') or key == ord('I'):
                self.show_landmarks_info = not self.show_landmarks_info
                print(f"é—œéµé»è³‡è¨Š: {'é–‹å•Ÿ' if self.show_landmarks_info else 'é—œé–‰'}")
            elif key == ord('r') or key == ord('R'):
                if self.video_writer is None:
                    self.start_video_recording(frame.shape)
                else:
                    self.stop_video_recording()
            elif key == ord('a') or key == ord('A'):
                self.auto_save = not self.auto_save
                print(f"è‡ªå‹•æˆªåœ–: {'é–‹å•Ÿ' if self.auto_save else 'é—œé–‰'}")
            elif key == ord(' '):  # ç©ºç™½éµ
                self.save_frame(frame, "manual_capture")
            elif key == ord('d') or key == ord('D'):
                self.save_detection_result_image(frame)
            elif key == ord('v') or key == ord('V'):
                print("âš ï¸  é–‹å§‹éŒ„è£½åµæ¸¬åˆ†æå½±ç‰‡ï¼Œè«‹ä¿æŒå§¿å‹¢...")
                self.create_detection_video(10)  # éŒ„è£½10ç§’
            elif key == ord('t') or key == ord('T'):
                print("ğŸ”§ åŸ·è¡ŒYOLOé™¤éŒ¯æ¸¬è©¦...")
                if self.yolo_model is not None:
                    self.debug_yolo_detection(frame)
                else:
                    print("âš ï¸  YOLOæ¨¡å‹æœªè¼‰å…¥ï¼Œå˜—è©¦é‡æ–°è¼‰å…¥...")
                    self.load_yolo_model()
            elif key == ord('x') or key == ord('X'):
                self.mirror_mode = not self.mirror_mode
                print(f"é¡åƒæ¨¡å¼: {'é–‹å•Ÿ' if self.mirror_mode else 'é—œé–‰'}")
                print("ğŸ’¡ æç¤ºï¼šé—œé–‰é¡åƒæ¨¡å¼å¯ä¿®æ­£éŒ„å½±æ™‚çš„å·¦å³ç›¸åå•é¡Œ")
            elif key >= ord('1') and key <= ord('5'):
                # èª¿æ•´å¹€è·³èºè¨­å®š
                self.skip_frames = int(chr(key))
                print(f"ğŸš€ å¹€è·³èºè¨­å®š: 1/{self.skip_frames} ({'æœ€é«˜å“è³ª' if self.skip_frames == 1 else 'æå‡æ€§èƒ½'})")
                if self.skip_frames > 1:
                    print("ğŸ’¡ æç¤ºï¼šæ•¸å­—è¶Šå¤§FPSè¶Šé«˜ï¼Œä½†åµæ¸¬æ›´æ–°é »ç‡è¶Šä½")
        
        # æ¸…ç†è³‡æº
        self.stop_video_recording()
        cap.release()
        cv2.destroyAllWindows()
        print("ç¨‹å¼å·²çµæŸ")
    
    def save_landmarks_data(self, frame, filename=None):
        """å„²å­˜æ‰€æœ‰é—œéµé»è³‡æ–™åˆ°æª”æ¡ˆ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"landmarks_data_{timestamp}.txt"
        
        filepath = os.path.join(self.output_dir, "data", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("Integrated Landmarks Data (MediaPipe + YOLOv11n-pose)\n")
            f.write("=" * 60 + "\n")
            f.write(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ‰‹éƒ¨è³‡æ–™
            hands_results = self.hands.process(rgb_frame)
            if hands_results.multi_hand_landmarks:
                for i, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):
                    f.write(f"Hand {i+1} (21 landmarks - MediaPipe):\n")
                    coords = self.get_landmarks_coordinates(hand_landmarks, frame.shape)
                    for j, (x, y, z) in enumerate(coords):
                        f.write(f"  {j:2d} {self.hand_landmarks_names[j]:20s}: ({x:4d}, {y:4d}, {z:6.3f})\n")
                    f.write("\n")
            
            # è‡‰éƒ¨è³‡æ–™
            face_results = self.face_mesh.process(rgb_frame)
            if face_results.multi_face_landmarks:
                f.write("Face Landmarks (248 points - MediaPipe):\n")
                coords = self.get_landmarks_coordinates(face_results.multi_face_landmarks[0], frame.shape)
                for j, (x, y, z) in enumerate(coords):
                    f.write(f"  {j:3d}: ({x:4d}, {y:4d}, {z:6.3f})\n")
                f.write("\n")
            
            # äººé«”å§¿æ…‹è³‡æ–™
            if self.yolo_model is not None:
                try:
                    pose_results = self.yolo_model(frame, verbose=False)
                    for result in pose_results:
                        if hasattr(result, 'keypoints') and result.keypoints is not None:
                            # è™•ç†ä¸åŒçš„keypointsæ ¼å¼
                            if hasattr(result.keypoints, 'data'):
                                keypoints_data = result.keypoints.data
                            elif hasattr(result.keypoints, 'xy'):
                                keypoints_data = result.keypoints.xy
                            else:
                                keypoints_data = result.keypoints
                            
                            if len(keypoints_data.shape) == 3:
                                keypoints = keypoints_data[0]
                            else:
                                keypoints = keypoints_data
                            
                            keypoints = keypoints.cpu().numpy() if hasattr(keypoints, 'cpu') else keypoints
                            
                            f.write("Pose Landmarks (17 points - YOLOv11n-pose):\n")
                            for j, point in enumerate(keypoints):
                                if len(point) >= 3:
                                    x, y, conf = point[0], point[1], point[2]
                                elif len(point) == 2:
                                    x, y, conf = point[0], point[1], 1.0
                                else:
                                    continue
                                
                                if conf > 0.3:
                                    f.write(f"  {j:2d} {self.pose_keypoints[j]:15s}: ({x:7.1f}, {y:7.1f}, conf:{conf:5.3f})\n")
                            f.write("\n")
                except Exception as e:
                    f.write(f"Pose detection error: {e}\n")
        
        print(f"ğŸ’¾ é—œéµé»è³‡æ–™å·²å„²å­˜è‡³: {filepath}")
        return filepath
    
    def get_landmarks_coordinates(self, landmarks, frame_shape):
        """ç²å–é—œéµé»åº§æ¨™"""
        h, w = frame_shape[:2]
        coordinates = []
        for landmark in landmarks.landmark:
            x = int(landmark.x * w)
            y = int(landmark.y * h)
            z = landmark.z if hasattr(landmark, 'z') else 0
            coordinates.append([x, y, z])
        return coordinates


def main():
    """ä¸»ç¨‹å¼"""
    print("æ­£åœ¨åˆå§‹åŒ–åµæ¸¬å™¨...")
    detector = IntegratedLandmarkDetector()
    
    print("\nMediaPipe + YOLOv11n-pose æ•´åˆåµæ¸¬å™¨")
    print("=" * 60)
    print("æ­¤ç¨‹å¼æ•´åˆäº†ä¸‰ç¨®å…ˆé€²çš„é—œéµé»åµæ¸¬æŠ€è¡“ï¼š")
    print("â€¢ MediaPipe æ‰‹éƒ¨åµæ¸¬ï¼š21å€‹é—œéµé»ï¼ˆæœ€å¤š2éš»æ‰‹ï¼Œå…±42å€‹é»ï¼‰")
    print("â€¢ MediaPipe è‡‰éƒ¨åµæ¸¬ï¼š248å€‹é—œéµé»")
    print("â€¢ YOLOv11n-pose äººé«”åµæ¸¬ï¼š17å€‹é—œéµé»")
    print("")
    
    try:
        detector.run_camera()
    except KeyboardInterrupt:
        print("\nç¨‹å¼è¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    main()